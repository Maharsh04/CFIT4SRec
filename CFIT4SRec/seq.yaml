## model config
#embedding_size: 32

# dataset config

# ---------------------------Amazon Dataset-----------------------------
# field_separator: "\t" # Specify the field delimiter of the dataset
# seq_separator: " " # Specify the delimiter for token_seq or float_seq fields in the dataset

# ---------------------------ML-1M dataset-----------------------------
field_separator: "::" # Specify the field delimiter of the dataset
seq_separator: "|" # Specify the delimiter for token_seq or float_seq fields in the dataset


# ---------------------------MIND dataset-----------------------------
# field_separator: "\t" # Specify the field delimiter of the dataset
# seq_separator: " " # Specify the delimiter for token_seq or float_seq fields in the dataset

USER_ID_FIELD: user_id # Specify the user ID field
ITEM_ID_FIELD: item_id # Specify the item ID field
# CLICKED_FIELD: clicked # Specify the Clicked field (For MIND Dataset)
RATING_FIELD: rating # Specify the rating field
TIME_FIELD: timestamp # Specify the timestamp field

# Specify from which file to read which columns. Here, it reads user_id, item_id, rating, timestamp from ml-1m.inter, and so on
#---------------------MovieLens Dataset---------------------
load_col:
    inter: [user_id, item_id, rating, timestamp]

#---------------------MIND Dataset-------------------------
# load_col:
#     inter: [user_id, item_id, clicked, timestamp]


NEG_PREFIX: neg_ # Specify the prefix for negative sampling
LABEL_FIELD: label # Specify the label field
ITEM_LIST_LENGTH_FIELD: item_length # Specify the field for sequence length
LIST_SUFFIX: _list # Specify the prefix for sequences
MAX_ITEM_LIST_LENGTH: 50 # Specify the maximum sequence length
POSITION_FIELD: position_id # Specify the generated sequence position ID

#max_user_inter_num: 100
min_user_inter_num: 5
#max_item_inter_num: 100
min_item_inter_num: 5
#lowest_val:
#    timestamp: 1546264800
#highest_val:
#    timestamp: 1577714400

# training settings
epochs: 20 # Maximum number of training epochs
train_batch_size: 256 # Batch size for training
learner: adam # PyTorch optimizer
learning_rate: 0.001 # Learning rate
training_neg_sample_num: 0 # Number of negative samples
eval_step: 1 # Evaluation frequency after each training step
stopping_step: 10 # Early stopping step count if the validation metric doesnâ€™t improve
# evaluation settings
eval_setting: TO_LS,full # Sort data by time, use leave-one-out data split, and full ranking
metrics: ["NDCG","Hit"] # Evaluation metrics
valid_metric: Hit@10 # Metric to determine early stopping
# eval_batch_size: 256 # Batch size for evaluation
eval_batch_size: 512 # Batch size for evaluation (For larger dataset like ML-1M)
weight_decay: 0
topk: [5, 10, 20]

# directory setting
log_root: "./log/"
data_path: "../dataset/"
#checkpoint_dir:

lmd: 0.1
lmd_sem: 0.1

tau: 1

# choose from {un, su, us, us_x}
contrast: 'us_x'

# choose from {dot, cos}
sim: 'dot'

hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5

loss_type : 'CE'

train_r : 1

noise : 'CLOSE'

noise_r : 0

same_length : 10

l_ok: True
h_ok: True
b_ok: True

low_r: False
high_r: False
